# 평가 (LLM-as-a-Judge)

모델 기반 평가( _LLM-as-a-judge_ )는 LangShark와 통합된 LLM 애플리케이션의 평가를 자동화하는 강력한 도구입니다. 모델 기반 평가를 통해 LLM은 정확성, 독성 또는 환각과 같은 기준에 따라 LangShark에서 특정 세션/추적/LLM 호출을 평가하는 데 사용됩니다.

### 어떤걸 평가할 수 있나요?

모델에대한 평가는 최종 End-to-End의 LLM 답변 평가, 외부에서 데이터를 참조하는경우 해당 외부지식에 대한 평가 두가지로 이뤄지게 됩니다.

#### LLM 답변 평가

AnswerRelevancy (답변 관련성)

* 생성된 답변이 주어진 질문에 얼마나 적절한지 평가

Hallucination (환각)

* 정보에 없는 내용을 생성하는지 평가

#### RAG 평가

Faithfulness (충실도)

* 생성된 답변이 검색된 컨텍스트에 얼마나 충실한지 평가

ContextualPrecision (컨텍스트 정확도)

* 검색된 컨텍스트가 얼마나 정확하고 관련 있는지 평가
* (목표) RAG의 검색 능력을 직접적으로 평가하며, 불필요한 정보 검색을 줄이는 것

ContextualRecall (컨텍스트 재현율)

* 필요한 모든 관련 정보가 검색되었는지 평가
* (목표) RAG 시스템이 필요한 정보를 누락 없이 검색하는 능력을 평가

### 어떻게 평가를 진행할 수 있나요?

외부 평가 파이프라인은 스마일샤크에서 직접 구축 혹은 관련내용 전달을 통해 Python SDK로 진행되게 됩니다.

해당 내용에 관해서는 스마일샤크에 문의 바랍니다.\
